{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb195293",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c9f63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f280320",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfca827",
   "metadata": {},
   "source": [
    "This is small dataset with question, good choice for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c9b14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(\"../data/quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a2294",
   "metadata": {},
   "source": [
    "## Example of training and using my word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b93e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = Path().resolve().parent\n",
    "sys.path.append(str(parent_folder))\n",
    "\n",
    "from word2vec import Word2VecSkipGram\n",
    "from word2vec import Word2VecSkipGramModel\n",
    "from word2vec import Word2VecCBOWModel\n",
    "from word2vec import Word2VecCBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd438c",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f081894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model CBOW\n",
    "cbow = Word2VecCBOW()\n",
    "\n",
    "# set data - this step save data, tokenize it and \n",
    "# create word_to_index and index_to_word dictionaries\n",
    "cbow.set_text_before_context_pairs(data)\n",
    "\n",
    "# set context groups and model - this step create groups\n",
    "# [center_word, context_word_1, ... context_word_{window_radius * 2}]\n",
    "# and set model for training\n",
    "cbow.set_context_groups_and_model()\n",
    "\n",
    "# subsampling probabilities - this step calculate probabilities\n",
    "# for each word to be deleted from training. I use formula from\n",
    "# paper:\n",
    "# \n",
    "# P(w_i) = 1 - sqrt(t / f(w_i))\n",
    "# \n",
    "# where t is threshold and f(w_i) is frequency of w_i in the dataset\n",
    "cbow.subsampling_probabilities()\n",
    "\n",
    "# negative sampling probabilities - this step calculate probabilities\n",
    "# for each word to be used as negative sample. I use formula from\n",
    "# paper:\n",
    "# \n",
    "# P(w_i) = (f(w_i) / sum(f(w_j))) ^ (3/4) / Z\n",
    "# \n",
    "# where f(w_i) is frequency of w_i in the dataset and Z is normalization constant\n",
    "cbow.negative_sampling_probabilities()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4ef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/10001 [00:00<10:03, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 11.091211318969727, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1003/10001 [00:51<07:48, 19.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000, Loss: 3.510890483856201, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2004/10001 [01:44<06:44, 19.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000, Loss: 3.0968456268310547, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3004/10001 [02:37<05:57, 19.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000, Loss: 3.0199851989746094, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4004/10001 [03:30<05:26, 18.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000, Loss: 2.773721694946289, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4040/10001 [03:31<05:03, 19.62it/s]"
     ]
    }
   ],
   "source": [
    "# Let's train the model\n",
    "cbow.train_model(steps=10001, batch_size=128, negative_number=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb7bd4",
   "metadata": {},
   "source": [
    "Loss is not decreasing, but its normal - if you will train 2000 steps and 10000 steps, the difference will be huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_parameters = cbow.model.parameters()\n",
    "embedding_matrix_center = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that first matrix was for central word\n",
    "embedding_matrix_context = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that second matrix was for co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6398c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hadoop', 0.7932560443878174),\n",
       " ('spring', 0.7990931868553162),\n",
       " ('design', 0.8000043034553528),\n",
       " ('photoshop', 0.8032585978507996),\n",
       " ('algorithms', 0.8079955577850342),\n",
       " ('seo', 0.8176975250244141),\n",
       " ('php', 0.8207674026489258),\n",
       " ('c', 0.8241156339645386),\n",
       " ('java', 0.8498246669769287),\n",
       " ('python', 0.9999999403953552)]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_nearest(words):\n",
    "    word_vector = cbow.get_center_embeddings_by_words(words)\n",
    "    dists = F.cosine_similarity(embedding_matrix_center, word_vector)\n",
    "    index_sorted = torch.argsort(dists)\n",
    "    top_k = index_sorted[-10:]\n",
    "    return [(cbow.index_to_word[x], dists[x].item()) for x in top_k.numpy()]\n",
    "find_nearest([\"python\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be92bb0",
   "metadata": {},
   "source": [
    "### Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714aba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model CBOW\n",
    "skipgram = Word2VecSkipGram()\n",
    "\n",
    "# set data - this step save data, tokenize it and \n",
    "# create word_to_index and index_to_word dictionaries\n",
    "skipgram.set_text_before_context_pairs(data)\n",
    "\n",
    "# set context groups and model - this step create groups\n",
    "# [center_word, context_word_1, ... context_word_{window_radius * 2}]\n",
    "# and set model for training\n",
    "skipgram.set_context_groups_and_model()\n",
    "\n",
    "# subsampling probabilities - this step calculate probabilities\n",
    "# for each word to be deleted from training. I use formula from\n",
    "# paper:\n",
    "# \n",
    "# P(w_i) = 1 - sqrt(t / f(w_i))\n",
    "# \n",
    "# where t is threshold and f(w_i) is frequency of w_i in the dataset\n",
    "skipgram.subsampling_probabilities()\n",
    "\n",
    "# negative sampling probabilities - this step calculate probabilities\n",
    "# for each word to be used as negative sample. I use formula from\n",
    "# paper:\n",
    "# \n",
    "# P(w_i) = (f(w_i) / sum(f(w_j))) ^ (3/4) / Z\n",
    "# \n",
    "# where f(w_i) is frequency of w_i in the dataset and Z is normalization constant\n",
    "skipgram.negative_sampling_probabilities()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390fb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the model\n",
    "skipgram.train_model(steps=10001, batch_size=128, negative_number=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43957678",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_parameters = skipgram.model.parameters()\n",
    "embedding_matrix_center = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that first matrix was for central word\n",
    "embedding_matrix_context = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that second matrix was for co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(words):\n",
    "    word_vector = skipgram.get_center_embeddings_by_words(words)\n",
    "    dists = F.cosine_similarity(embedding_matrix_center, word_vector)\n",
    "    index_sorted = torch.argsort(dists)\n",
    "    top_k = index_sorted[-10:]\n",
    "    return [(skipgram.index_to_word[x], dists[x].item()) for x in top_k.numpy()]\n",
    "find_nearest([\"python\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
