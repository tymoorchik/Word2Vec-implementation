{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb195293",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9f63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f280320",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfca827",
   "metadata": {},
   "source": [
    "This is small dataset with question, good choice for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c9b14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(\"data/quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a2294",
   "metadata": {},
   "source": [
    "## Example of training and using my word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493f7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# os.chdir(os.path.expanduser(\"~\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec.Word2VecSkipGram import Word2VecSkipGram\n",
    "from word2vec.Word2VecSkipGram import Word2VecSkipGramModel\n",
    "from word2vec.Word2VecCBOW import Word2VecCBOWModel\n",
    "from word2vec.Word2VecCBOW import Word2VecCBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd438c",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f081894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model CBOW\n",
    "cbow = Word2VecCBOW()\n",
    "\n",
    "# set data - this step save data, tokenize it and \n",
    "# create word_to_index and index_to_word dictionaries\n",
    "cbow.set_text_before_context_pairs(data)\n",
    "\n",
    "# set context groups and model - this step create groups\n",
    "# [center_word, context_word_1, ... context_word_{window_radius * 2}]\n",
    "# and set model for training\n",
    "cbow.set_context_groups_and_model()\n",
    "\n",
    "# subsampling probabilities - this step calculate probabilities\n",
    "# for each word to be deleted from training. I use formula from\n",
    "# paper:\n",
    "# \n",
    "# P(w_i) = 1 - sqrt(t / f(w_i))\n",
    "# \n",
    "# where t is threshold and f(w_i) is frequency of w_i in the dataset\n",
    "cbow.subsampling_probabilities()\n",
    "\n",
    "# negative sampling probabilities - this step calculate probabilities\n",
    "# for each word to be used as negative sample. I use formula from\n",
    "# paper:\n",
    "# \n",
    "# P(w_i) = (f(w_i) / sum(f(w_j))) ^ (3/4) / Z\n",
    "# \n",
    "# where f(w_i) is frequency of w_i in the dataset and Z is normalization constant\n",
    "cbow.negative_sampling_probabilities()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bf4ef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/10001 [00:00<10:03, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 11.091211318969727, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1003/10001 [00:51<07:48, 19.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000, Loss: 3.510890483856201, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2004/10001 [01:44<06:44, 19.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000, Loss: 3.0968456268310547, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3004/10001 [02:37<05:57, 19.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000, Loss: 3.0199851989746094, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4004/10001 [03:30<05:26, 18.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000, Loss: 2.773721694946289, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5003/10001 [04:21<04:09, 20.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000, Loss: 2.8424835205078125, learning rate: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6004/10001 [05:12<03:22, 19.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000, Loss: 2.58914852142334, learning rate: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7004/10001 [06:03<02:31, 19.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7000, Loss: 2.610912799835205, learning rate: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8004/10001 [06:54<01:41, 19.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8000, Loss: 2.727409839630127, learning rate: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9004/10001 [07:45<00:53, 18.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9000, Loss: 2.714916229248047, learning rate: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [08:37<00:00, 19.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10000, Loss: 2.436723232269287, learning rate: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's train the model\n",
    "cbow.train_model(steps=10001, batch_size=128, negative_number=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb7bd4",
   "metadata": {},
   "source": [
    "Loss is not decreasing, but its normal - if you will train 2000 steps and 10000 steps, the difference will be huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4bb7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_parameters = cbow.model.parameters()\n",
    "embedding_matrix_center = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that first matrix was for central word\n",
    "embedding_matrix_context = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that second matrix was for co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e6398c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('finance', 0.7676918506622314),\n",
       " ('coding', 0.7689043283462524),\n",
       " ('frontend', 0.7759082913398743),\n",
       " ('sql', 0.7793285846710205),\n",
       " ('django', 0.7799999713897705),\n",
       " ('javascript', 0.8302862644195557),\n",
       " ('php', 0.8529316186904907),\n",
       " ('java', 0.8859761953353882),\n",
       " ('c', 0.9090428352355957),\n",
       " ('python', 1.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_nearest(words):\n",
    "    word_vector = cbow.get_center_embeddings_by_words(words)\n",
    "    dists = F.cosine_similarity(embedding_matrix_center, word_vector)\n",
    "    index_sorted = torch.argsort(dists)\n",
    "    top_k = index_sorted[-10:]\n",
    "    return [(cbow.index_to_word[x], dists[x].item()) for x in top_k.numpy()]\n",
    "find_nearest([\"python\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e55c00",
   "metadata": {},
   "source": [
    "Python is near with another languages of programming! Succes!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be92bb0",
   "metadata": {},
   "source": [
    "### Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "714aba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model CBOW\n",
    "skipgram = Word2VecSkipGram()\n",
    "\n",
    "# set data - this step save data, tokenize it and \n",
    "# create word_to_index and index_to_word dictionaries\n",
    "skipgram.set_text_before_context_pairs(data)\n",
    "\n",
    "# set context groups and model - this step create groups\n",
    "# [center_word, context_word_1, ... context_word_{window_radius * 2}]\n",
    "# and set model for training\n",
    "skipgram.set_context_groups_and_model()\n",
    "\n",
    "# subsampling probabilities - this step calculate probabilities\n",
    "# for each word to be deleted from training. I use formula from\n",
    "# paper:\n",
    "# \n",
    "# P(w_i) = 1 - sqrt(t / f(w_i))\n",
    "# \n",
    "# where t is threshold and f(w_i) is frequency of w_i in the dataset\n",
    "skipgram.subsampling_probabilities()\n",
    "\n",
    "# negative sampling probabilities - this step calculate probabilities\n",
    "# for each word to be used as negative sample. I use formula from\n",
    "# paper:\n",
    "# \n",
    "# P(w_i) = (f(w_i) / sum(f(w_j))) ^ (3/4) / Z\n",
    "# \n",
    "# where f(w_i) is frequency of w_i in the dataset and Z is normalization constant\n",
    "skipgram.negative_sampling_probabilities()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390fb6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/10001 [00:00<14:36, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 11.090789794921875, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1004/10001 [00:51<07:36, 19.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000, Loss: 4.32220983505249, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2003/10001 [01:44<06:44, 19.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000, Loss: 3.7768969535827637, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3004/10001 [02:36<06:08, 19.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000, Loss: 3.6793813705444336, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4004/10001 [03:27<05:01, 19.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000, Loss: 3.430278778076172, learning rate: [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5004/10001 [04:18<04:10, 19.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000, Loss: 3.496466636657715, learning rate: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6003/10001 [05:09<04:05, 16.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000, Loss: 3.4756500720977783, learning rate: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7004/10001 [06:01<02:29, 20.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7000, Loss: 3.4848098754882812, learning rate: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8003/10001 [06:51<01:40, 19.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8000, Loss: 3.3417301177978516, learning rate: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9004/10001 [07:42<00:53, 18.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9000, Loss: 3.298994779586792, learning rate: [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [08:33<00:00, 19.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10000, Loss: 3.2748324871063232, learning rate: [0.00025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's train the model\n",
    "skipgram.train_model(steps=10001, batch_size=128, negative_number=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf74af4",
   "metadata": {},
   "source": [
    "Loss is not decreasing, but its normal - if you will train 2000 steps and 10000 steps, the difference will be huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43957678",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_parameters = skipgram.model.parameters()\n",
    "embedding_matrix_center = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that first matrix was for central word\n",
    "embedding_matrix_context = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that second matrix was for co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b50aef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cantonese', 0.6946611404418945),\n",
       " ('grammar', 0.6997422575950623),\n",
       " ('scratch', 0.7020895481109619),\n",
       " ('language', 0.7183550596237183),\n",
       " ('programming', 0.7199528217315674),\n",
       " ('basics', 0.7256700992584229),\n",
       " ('javascript', 0.7391355633735657),\n",
       " ('java', 0.7444342374801636),\n",
       " ('c', 0.7820838689804077),\n",
       " ('python', 1.0000001192092896)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_nearest(words):\n",
    "    word_vector = skipgram.get_center_embeddings_by_words(words)\n",
    "    dists = F.cosine_similarity(embedding_matrix_center, word_vector)\n",
    "    index_sorted = torch.argsort(dists)\n",
    "    top_k = index_sorted[-10:]\n",
    "    return [(skipgram.index_to_word[x], dists[x].item()) for x in top_k.numpy()]\n",
    "find_nearest([\"python\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789a8be",
   "metadata": {},
   "source": [
    "Also python with languages!!! Succes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
